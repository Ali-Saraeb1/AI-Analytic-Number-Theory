{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cd83ad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# This code trains a lightgbm classifier and tests it on a test dataset of size 214. The model achieves 0.939 test accuracy and 0.967 validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0977bd92",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "from scipy.stats import skew, kurtosis\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, log_loss)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm import LGBMClassifier\n",
    "import joblib\n",
    "from lightgbm import early_stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af0e7bc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Data_Dir = Path(\"/content/drive/MyDrive/Dirichlet_Mod_Cond\")\n",
    "Train_Dir = Data_Dir / \"Zeros_per_Modulus_Train\"\n",
    "Test_Dir = Data_Dir / \"Zeros_per_Modulus_Test\"\n",
    "Out_Dir = Data_Dir / \"results\"\n",
    "\n",
    "Seed = 42\n",
    "FFT_Comp = 30\n",
    "Test_Sz = 0.2 # this is for validation set contruction purposes\n",
    "Early_Stoping_Rounds = 75\n",
    "\n",
    "\n",
    "# learning rate was tuned after several experiements\n",
    "\n",
    "Model_Params = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_leaves\": 127,\n",
    "    \"random_state\": Seed,\n",
    "    \"num_class\": 140,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"class_weight\": \"balanced\",\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"n_estimators\": 1500,\n",
    "    \"feature_fraction\": 0.85,\n",
    "    \"bagging_fraction\": 0.85,\n",
    "    \"class_weight\": \"balanced\",\n",
    "    \"n_jobs\": -1,\n",
    "    \"verbose\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f53497",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# each text document is named q (an integer that is the label i.e the modulus)\n",
    "Fixed_Zero_Count = 25  # use exactly first 25 zeros\n",
    "\n",
    "def load_data(folder: Path) -> pd.DataFrame:\n",
    "\n",
    "    pattern = re.compile(r\"[-+]?\\d*\\.\\d+(?:[eE][-+]?\\d+)?\")\n",
    "    label_zeros = []\n",
    "\n",
    "    for file_path in folder.glob(\"*.txt\"):\n",
    "        label = int(file_path.stem)\n",
    "        with file_path.open() as f:\n",
    "            for line in f:\n",
    "                parts = line.split(':', maxsplit=1)\n",
    "                zero_values = pattern.findall(parts[1])\n",
    "                zeros = [float(z) for z in zero_values]\n",
    "                label_zeros.append([label] + zeros)\n",
    "\n",
    "    columns = [\"label\"] + [f\"zero_{i+1}\" for i in range(Fixed_Zero_Count)]\n",
    "    return pd.DataFrame(label_zeros, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dadf96f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#  statistical moments\n",
    "# first and second differences\n",
    "# FFT magnitudes (first FFT_Comp components)\n",
    "def extract_features(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    feature_list = []\n",
    "\n",
    "    for row in data.itertuples(index=False):\n",
    "        label = int(row.label)\n",
    "        zeros = np.array(row[1:], dtype=float)\n",
    "\n",
    "        #sample moments\n",
    "        mean_z = nonzero.mean() \n",
    "        var_z = nonzero.var() \n",
    "        skew_z = skew(nonzero) \n",
    "        kurt_z = kurtosis(nonzero) \n",
    "        rms_z = np.sqrt((nonzero**2).mean()) \n",
    "\n",
    "        # Differences\n",
    "        diff1 = np.diff(nonzero) \n",
    "        diff2 = np.diff(diff1) \n",
    "        mean_d1, var_d1 = diff1.mean(), diff1.var()\n",
    "        mean_d2, var_d2 = diff2.mean(), diff2.var()\n",
    "\n",
    "        # FFT features\n",
    "        fft_vals = np.fft.rfft(nonzero)\n",
    "        mags = np.abs(fft_vals)[1 : FFT_Comp + 1]\n",
    "        if mags.size < FFT_Comp:\n",
    "            mags = np.pad(mags, (0, FFT_Comp - mags.size))\n",
    "\n",
    "        features = [\n",
    "            label, mean_z, var_z, skew_z, kurt_z, rms_z,\n",
    "            mean_d1, var_d1, mean_d2, var_d2, mean_pg,\n",
    "            *mags.tolist(),\n",
    "        ]\n",
    "        feature_list.append(features)\n",
    "\n",
    "    # Column names\n",
    "    stats_cols = [\n",
    "        \"label\", \"mean_z\", \"var_z\", \"skew_z\", \"kurt_z\", \"rms_z\",\n",
    "        \"mean_d1\", \"var_d1\", \"mean_d2\", \"var_d2\", \"mean_pg\",\n",
    "    ]\n",
    "    fft_cols = [f\"FFT_{i+1}\" for i in range(FFT_Comp)]\n",
    "\n",
    "    return pd.DataFrame(feature_list, columns=stats_cols + fft_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8239a030",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def compute_and_save_metrics(y_true, y_pred, y_proba, classes, tag):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    ll = log_loss(y_true, y_proba, labels=classes)\n",
    "    logging.info(f\"{tag}: Accuracy={acc:.4f}, LogLoss={ll:.4f}\")\n",
    "    print(f\"[{tag}] Accuracy: {acc:.4f}, LogLoss: {ll:.4f}\")\n",
    "\n",
    "    # probabilities and true vs predicted\n",
    "    df_probs = pd.DataFrame(y_proba, columns=classes)\n",
    "    df_probs.insert(0, \"predicted\", y_pred)\n",
    "    df_probs.insert(0, \"true\", y_true.values)\n",
    "    df_probs.to_csv(Out_Dir / f\"{tag}_probs.csv\", index=False)\n",
    "\n",
    "    df_tp = pd.DataFrame({\"true\": y_true.values, \"predicted\": y_pred})\n",
    "    df_tp.to_csv(Out_Dir / f\"{tag}_true_vs_pred.csv\", index=False)\n",
    "\n",
    "    pd.DataFrame([{\"accuracy\": acc, \"log_loss\": ll}]) \\\n",
    "      .to_csv(Out_Dir / f\"{tag}_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34da896b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_data = load_data(Train_Dir)\n",
    "    test_data = load_data(Test_Dir)\n",
    "\n",
    "    train_features = extract_features(train_data)\n",
    "    test_features = extract_features(test_data)\n",
    "\n",
    "    X_train = train_features.drop(columns=[\"label\"])\n",
    "    y_train = train_features[\"label\"]\n",
    "    X_test = test_features.drop(columns=[\"label\"])\n",
    "    y_test = test_features[\"label\"]\n",
    "\n",
    "    # Train-validation set split\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=Test_Sz, stratify=y_train, random_state=Seed,)\n",
    "\n",
    "    scaler = StandardScaler().fit(X_tr)\n",
    "    X_tr_scaled = scaler.transform(X_tr)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # train\n",
    "    model = LGBMClassifier(**Model_Params)\n",
    "    model.fit(X_tr_scaled, y_tr, eval_set=[(X_val_scaled, y_val)], eval_metric=\"multi_logloss\", callbacks=[early_stopping(stopping_rounds=Early_Stoping_Rounds)],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110cc006",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for split, X_split, y_split in [(\"Val\", X_val_scaled, y_val), (\"Test\", X_test_scaled, y_test)]:\n",
    "        preds = model.predict(X_split)\n",
    "        probs = model.predict_proba(X_split)\n",
    "        compute_and_save_metrics(y_split, preds, probs, model.classes_, split)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
